![logo bsg](images/logobsg.png)

# Curso Ingeniería de Datos en Plataformas en la Nube y Data Warehousing con Python - Sesion 8

## 3. Introducción a BigQuery para Data Warehousing (GCP)
BigQuery es el servicio de data warehouse de Google Cloud. Es una plataforma serverless de análisis de datos a gran escala que permite ejecutar consultas SQL ultra rápidas sobre petabytes de datos sin necesidad de administrar servidores ni estructuras de indexado manual. BigQuery se destaca por separar completamente el almacenamiento y cómputo: los datos se almacenan de forma distribuida en la infraestructura de Google, y cuando se ejecuta una consulta BigQuery asigna automáticamente recursos (slots de procesamiento) para escanear los datos de forma paralela.  

En BigQuery, no manejamos la noción de cluster o nodos como en Redshift; en su lugar, trabajamos con proyectos, datasets y tablas, y las consultas son ejecutadas por el motor de BigQuery "on demand". Esto simplifica la administración: es casi libre de configuración – uno carga los datos y lanza las consultas en SQL, y Google se encarga de la escalabilidad.

### 3.1 Cargando datos y ejecutando consultas en BigQuery
Estructura básica: Dentro de un proyecto GCP, BigQuery organiza los datos en datasets (conjunto de tablas, análogo a un esquema en bases de datos tradicionales). Cada dataset contiene tablas y/o views. Podemos cargar datos a BigQuery de varias formas: - Archivos en GCS: Se puede cargar CSV/JSON/Parquet/Avro desde Cloud Storage usando la UI, comandos bq de la CLI, o la API. - Streaming API: para ingestión continua de registros (hasta 100K filas por segundo, aunque en sandbox no está disponible). - Integraciones (Data Transfer Service): cargar datos desde Google Analytics, Ads, etc. - BigQuery Web UI: arrastrar CSVs pequeños directamente para pruebas. - Python client: usando la librería google-cloud-bigquery se puede programar la carga (inserción de filas o job de carga).  

Para este enfoque práctico, consideremos cargar un dataset público o de ejemplo y luego consultarlo. GCP ofrece muchos datasets públicos listos para consultar (por ejemplo, datos meteorológicos, taxis NYC, GitHub archive, etc.), lo cual es genial para practicar sin tener que cargar datos propios.  

**Ejemplo de consulta en Python:** Usando la librería Python oficial:  
```python
from google.cloud import bigquery

client = bigquery.Client()  # Usa credenciales configuradas (ADC)
query = """
SELECT name, state, SUM(number) as total
FROM `bigquery-public-data.usa_names.usa_1910_2013`
WHERE state IN ('TX','NY') AND year >= 2000
GROUP BY name, state
ORDER BY total DESC
LIMIT 10
"""
query_job = client.query(query)  # lanza la consulta
results = query_job.result()     # espera a que termine y obtiene resultado
for row in results:
    print(row.name, row.state, row.total)
```
En este código, consultamos una tabla pública de nombres de bebés en EE.UU., filtrando por ciertos estados y años, agrupando y ordenando por el total (básicamente, top 10 nombres más usados desde 2000 en Texas y Nueva York). Fíjate que referenciamos la tabla con un identificador completo project.dataset.table. BigQuery permite consultas federadas a datos públicos con solo tener un proyecto GCP activo (no cuesta nada consultar hasta 1 TB/mes dentro del free tier, como veremos).  

Bajo el capó, BigQuery está leyendo posiblemente millones de filas en paralelo, aprovechando la infraestructura masiva de Google. El usuario no ve esta complejidad: simplemente obtiene el resultado. El tiempo de consulta en BigQuery para datasets de cientos de GB suele ser de segundos, gracias a ese paralelismo y a la optimización interna (columnares, árboles agregados, etc.).  

**_Uso interactivo (BigQuery UI):_** Además de Python, uno puede ir a la consola web BigQuery, escribir consultas SQL en el editor, y ver resultados en la interfaz. Es común iterar allí durante exploración, y luego integrar las consultas en scripts Python para automatización.  

**_Carga de datos de ejemplo:_** Supongamos que queremos cargar nuestros datos de ventas retail (similar al ejemplo de Redshift). Podríamos subir el CSV a un bucket GCS (como hicimos) y luego usar un comando SQL DDL en BigQuery o la UI para cargarlo, e.g.:

```sql
LOAD DATA 
INTO `mi_proyecto.mi_dataset.ventas`
FROM FILES("gs://mi-bucket-ejemplo-gcp/datos/ventas2025.csv")
FORMAT = CSV
OPTIONALLY ENCLOSED BY '"'
IGNORE 1 LINES;
```
**(Nota: BigQuery también permite hacer Create Table ... as Select directamente leyendo de Cloud Storage, o usar la CLI bq load).**  

Una vez en BigQuery, los datos se almacenan en formato columnar optimizado (Capacitor, el formato interno de BigQuery). Las consultas subsecuentes se benefician de esa estructura en columnas.  

**_BigQuery y SQL estándar:_** BigQuery soporta SQL estándar ANSI con extensiones. Permite tipos anidados (STRUCT, ARRAY) que son útiles para datos semiestructurados. Por ejemplo, JSON se puede mapear a tipos anidados y consultar con sintaxis de punto. Para la mayoría de usos, escribir SQL en BigQuery es familiar. Una diferencia es que BigQuery es case-insensitive en identificadores pero sensible a mayúsculas/minúsculas en comparaciones de strings por defecto (viene de su legado). También ciertas funciones analíticas o definiciones de ventanas están disponibles.  

**_Costo y Free Tier:_** BigQuery tiene un modelo de costo pay-as-you-go muy diferente a Redshift: - Se cobra principalmente por datos procesados en consultas (por default $5 por TB procesado). Sin embargo, los primeros 1 TB por mes son gratuitos dentro del free tier. Esto es bastante generoso para un estudiante: uno puede ejecutar muchas consultas de prueba siempre que no superen 1 TB leído acumulado mensual (y para contexto, 1 TB es 1,000 GB de datos escaneados; si cada consulta lee 1 GB, podrías hacer 1000 consultas en el mes gratis). - El almacenamiento en BigQuery se cobra aparte a ~$0.02 por GB por mes (activos) después de 10 GB. Pero los primeros 10 GB de almacenamiento por mes son gratis. Entonces, puedes almacenar datasets pequeños sin costo. Además, BigQuery tiene un Sandbox que habilita estos límites sin requerir ni siquiera cuenta de facturación, ideal para aprender sin riesgo. - No hay costo por ingreso de datos ni por exportar datos. - Importante: BigQuery no cobra por resultados cacheados. Si corres exactamente la misma consulta dos veces y los datos no cambiaron, la segunda vez detectará resultado en cache y no te cobrará nada y responderá rápidamente. Esto es bueno para iteración en análisis o dashboards refrescando: repetir consulta dentro de 24 horas con mismo texto se sirve del cache gratuitamente (a menos que se desactive a propósito).  

**_Ejemplo práctico (escenario Finanzas, consultas):_** supongamos que cargamos a BigQuery transacciones financieras diarias (como hicimos con S3+Redshift). Una vez en BQ, un analista puede ejecutar: "¿Cuál es la suma de transacciones por tipo de tarjeta en el último trimestre?" con un simple SQL. BigQuery podría tener 500 millones de registros del trimestre (~varios GB). Al ser una consulta agregada sobre toda la tabla, BigQuery efectivamente escaneará esos varios GB de datos – supongamos 5 GB – lo cual costaría alrededor de $0.025 (muy barato) y tardaría quizá 5-10 segundos en completarse, dado que utiliza decenas de miles of CPUs en paralelo si es necesario. Y como está dentro del free tier (<1 TB), no costaría nada en la práctica.  

### 3.2 Optimización de consultas y manejo de grandes volúmenes en BigQuery
Aunque BigQuery es administrado automáticamente, como usuarios de data warehouse debemos considerar mejores prácticas para rendimiento y costo, especialmente con grandes volúmenes de datos. Aquí entran en juego dos conceptos clave: Partitioning (particionado) y Clustering (clusterización) de tablas, muy similares en objetivo a las distribuciones y sort keys de Redshift, pero implementados a nivel del servicio.  

* **_Particionado de tablas:_** BigQuery permite particionar nativamente una tabla por una columna de fecha/timestamp, o por rango entero, o por ingestión de fecha. Particionar significa que la tabla se segmenta en “sub-tablas” lógicas por valor (ej. una partición por día). ¿Por qué es útil? Porque BigQuery puede prunear (omitir) particiones enteras en una consulta, escaneando solo las necesarias. Ejemplo: Una tabla de logs web particionada por fecha. Si ejecutamos `SELECT * FROM logs WHERE date = '2025-12-10'`, BigQuery leerá solo la partición del 10 de diciembre 2025, ignorando los otros 364 días, lo que reduce drásticamente los datos leídos y acelera la consulta. Esto reduce costos y tiempo. BigQuery incluso recomienda habilitar `require_partition_filter=true` al definir la tabla particionada para obligar a que las consultas especifiquen filtrado de partición, evitando accidentalmente escanear todo y gastar de más.  
    * Ejemplo: En un caso de IoT (dominio salud, monitorización de dispositivos médicos), podríamos particionar los datos de lecturas por fecha. Si tenemos 5 años de datos pero normalmente analizamos solo la última semana, las consultas rutinarias escanearán solo ~7 particiones en vez de 1825, ahorrando ~99.6% del costo comparado con no particionar.
* **_Clusterización de tablas:_** La clusterización en BigQuery es un método para ordenar físicamente los datos dentro de cada partición en base a campos específicos (hasta cuatro campos). Cuando se hace clustering por, digamos, `cliente_id`, BigQuery agrupa juntos en almacenamiento aquellos registros con valores de `cliente_id` similares. Entonces, si una consulta filtra por `cliente específico`, BigQuery puede saltar gran parte de los datos incluso dentro de la partición, porque sabe que los datos están agrupados. Clusterizar mejora el rendimiento de `filtros selectivos` y `joins en esa columna`, y también optimiza la compresión. Importante: A diferencia de un índice tradicional, no necesitas mantener nada – BigQuery reordena los datos en segundo plano.  
    * Ejemplo: Supongamos una tabla de ventas particionada por mes y además clusterizada por store_id. Si hacemos una consulta de un mes específico y la tienda X, BigQuery primero restringe a la partición de ese mes (reducción mayor), y luego dentro de ella tiene los datos de tienda X contiguos, así que solo lee unos pocos bloques de almacenamiento en lugar de toda la partición[40][44]. Un benchmark del propio Google en documentación indica que clusterizar adecuadamente puede reducir datos leídos en órdenes de magnitud si los filtros son selectivos.    
* **_Formato de almacenamiento y columnas:_** BigQuery almacena datos en columnas comprimidas, similar a Redshift. Por eso también conviene seleccionar solo columnas necesarias – menos datos leídos equivale a menos coste. A diferencia de Redshift, no controlamos la compresión ni sort keys manualmente; BigQuery maneja esto pero uno lo ayuda con clusterización. Evitar `SELECT *` en producción es recomendable tanto por costo como por eficiencia.  

* **_Denormalización vs Joins:_** A diferencia de un data warehouse tradicional, en BigQuery a veces se recomienda almacenar datos denormalizados (repetir ciertas dimensiones en la tabla de hechos) para evitar joins costosos, dado que BigQuery cobra por datos procesados (un join de dos tablas grandes procesará ambas). Esto va en contra de la teoría relacional, pero en la práctica BigQuery puede manejar tablas anchas eficientemente. De todos modos, BigQuery sí soporta joins, y con clusterización/distribución interna suele manejarlos bien, pero es un paradigma ligeramente distinto: los costos directos hacen que muchos diseñen con cierta redundancia controlada para consultas más simples.  

* **_Caching y resultados intermedios:_** Como mencionamos, BigQuery cachea resultados de consultas por 24 horas. Aprovechar esto es útil. Por ejemplo, si tienes un dashboard que refresca cada 30 minutos la misma consulta del último día, solo la primera del día gastará recursos; las siguientes 47 veces será cache hit (si los datos no cambiaron), costo $0. Eso es sin ningún trabajo extra del usuario, solo hace falta que la consulta sea exactamente igual en texto.  

* **_Limitando datos en consultas ad-hoc:_** Para exploración, conviene usar `LIMIT` o herramientas de preview para no escanear sin querer billones de filas. BigQuery indica en su UI cuántos bytes va a procesar una consulta antes de ejecutarla – es buena práctica mirar eso y asegurarse de que suena razonable.  

* **_Uso de tablas particionadas externamente:_** BigQuery puede también consultar datos sin cargarlos mediante external tables o funciones llamadas BigQuery federado. Por ejemplo, puedes definir una tabla externa sobre archivos en Cloud Storage (CSV o Parquet). Esto es útil para no duplicar datos, pero suele ser menos eficiente que cargarlo nativamente (porque BigQuery no almacena los datos en columnar interno). Sin embargo, es similar al Spectrum de Redshift – muy útil para ciertos casos como integración rápida o datos que ya están en GCS y no requieren transformaciones complejas. Un ejemplo: analizando un gran archivo CSV en GCS con SQL directamente definiendo un external table.  

**_Ejemplo práctico (escenario Retail, optimización):_** Tenemos en BigQuery una tabla `ventas` con 5 años de datos (~500 GB). Sin particionar, una consulta anual escanearía ~100 GB quizás. Decidimos particionar por año_mes (ej: 2025-12) y clusterizar por tienda_id. Ahora, la consulta "ventas de enero 2023 en la tienda 42" hará que BigQuery lea solo la partición 2023-01 (antes habría leído 5 años) y dentro de esa partición, gracias al clustering por tienda_id, apenas leerá bloques relacionados a la tienda 42, en lugar de todo enero. Esto podría reducir los datos escaneados de 500 GB a, digamos, 1 GB o menos, acelerando la respuesta y ahorrando dinero. Además, definimos la tabla con `require_partition_filter` para prevenir queries sin filtro temporal.  

**_Otro ejemplo:_** Un hospital guarda datos de pacientes y visitas en BigQuery. Cada paciente tiene múltiples visitas con datos clínicos. BigQuery soporta columnas anidadas `(ARRAY/STRUCT)`, por lo que se podría almacenar todas las visitas de un paciente en una estructura anidada dentro de la tabla de pacientes, evitando una tabla separate de visitas. Esto mejora ciertas consultas (no se hace join, se extrae del struct) y BigQuery las maneja eficientemente. Esto muestra que en BigQuery a veces se modela diferente a un data warehouse tradicional.  

BigQuery BI Engine y herramientas: Para casos de BI interactivo en dashboards de alta concurrencia, GCP ofrece BI Engine que es un caché en memoria integrado a BigQuery para acelerar consultas repetitivas en datasets medianos. Si una empresa en finanzas tiene dashboards que refrescan min a min, BI Engine puede mantener los últimos datos en RAM para respuesta sub-segundo. Mencionamos esto por completitud en un contexto de maestría, aunque su configuración escapa este resumen.  

Por último, recordar que BigQuery es serverless y automáticamente escalará recursos de cómputo según la complejidad de la consulta. No tenemos que preocuparnos por CPU o RAM; sin embargo, hay límites suaves (p.ej. máximo 6 MB de código SQL por consulta, 1000 particiones leídas max por tabla en query, etc.). En general, el enfoque debe ser en optimizar qué se lee más que cómo se procesa, dado que el costo y tiempo dependen principalmente de la cantidad de datos leídos del almacenamiento.  

### 3.3 Comparación final y aplicación en día a día con cuentas gratuitas
Tanto Redshift como BigQuery son soluciones potentes para warehousing en la nube, pero con filosofías distintas. Redshift se parece más a un motor de base de datos tradicional que administramos (aunque sea AWS quien gestiona muchas cosas tras bambalinas), mientras BigQuery es más “serverless” y orientado a simplificar al usuario final a costa de ceder control bajo el capó. Como especialistas en ambas nubes, es valioso saber que: - Integraciones: Redshift se integra nativamente con el ecosistema AWS (Glue, S3, Spectrum, AWS ML, etc.), mientras BigQuery juega muy bien con el ecosistema Google (Dataflow, Looker Studio, Google Sheets, etc.) y tiene conectores a servicios como Data Studio, etc. - Lenguajes y extensiones: Redshift admite UDFs (en SQL, y hasta hace poco en Python pero deprecándose) y procedimientos almacenados. BigQuery permite UDFs en SQL y JavaScript. BigQuery además tiene SQL geoespacial y ML integrado (BigQuery GIS, BigQuery ML) para entrenamiento de modelos sobre datos con simple SQL. - Costo predictivo: Redshift es costo fijo por hora (o por segundo en serverless) según tamaño de cluster, lo cual es bueno para cargas constantes y alto uso. BigQuery es costo variable por consulta, ideal para spiky workloads o para compartir un warehouse entre muchos equipos pagando solo por uso efectivo.  

En términos de aplicación diaria, un data engineer con Python puede programar pipelines ETL donde: - Los datos crudos se almacenan en S3/GCS (data lake). - Se ejecutan transformaciones quizás usando Spark o AWS Glue / GCP Dataflow. - Los resultados consolidados se cargan a Redshift/BigQuery para que analistas SQL puedan explotar la información. - Python (pandas, etc.) también puede hacer consultas directas: tanto Redshift como BigQuery tienen conectores para obtener resultados en dataframes, posibilitando análisis más avanzados o modelado de machine learning con scikit-learn sobre un subconjunto de datos traído del warehouse.  

**_Cuenta Free Tier (resumen):_** Para AWS, usar la prueba de Redshift (2 meses gratis) y S3 <=5 GB es ideal en una capacitación. Para GCP, BigQuery sandbox permite experimentar sin coste con hasta 10 GB almacenados y 1 TB procesado al mes[35], y Cloud Storage <=5 GB igualmente gratis. Esto significa que, por ejemplo, un laboratorio en clase podría incluir: - Crear un bucket S3 y GCS, subir un dataset público de ~100 MB. - Instanciar un Redshift (free trial) y cargar datos con COPY desde S3, luego consultar con Python y psycopg2/redshift_connector. - Cargar esos mismos datos a BigQuery (usando la web UI o API) y ejecutar la misma consulta agregada, comparando tiempos y sintaxis. - Los estudiantes no incurrirían en gastos reales siempre que los recursos se mantengan dentro de lo gratuito y se apaguen/eliminar clusters tras uso.  

Esta comparativa práctica les muestra cómo manejar almacenamiento y análisis de datos a gran escala en dos nubes líderes, utilizando Python como hilo conductor para automatizar tareas y ejecutar consultas. El objetivo es que demuestren la capacidad de gestionar y procesar datos a gran escala en entornos de nube, aplicando criterios técnicos (p.ej. elegir particiones, claves de distribución, optimizar consultas) y evaluando resultados (tiempos de consulta, costos aproximados) – justo las habilidades de un Data Engineer avanzado.