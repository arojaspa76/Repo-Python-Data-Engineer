![logo bsg](images/logobsg.png)

# Introducci√≥n a AWS Glue para Pipelines de Datos

# Secci√≥n 2: Automatizaci√≥n de Procesos de Datos con AWS Lambda

## üìã Objetivos de Aprendizaje

Al finalizar esta secci√≥n, ser√°s capaz de:
- Crear funciones Lambda para orquestar pipelines de datos
- Integrar Lambda con S3, Glue y otros servicios AWS
- Implementar triggers autom√°ticos basados en eventos
- Manejar errores y reintentos en procesos automatizados
- Optimizar costos usando Lambda dentro del Free Tier

---

## üí∞ Estimaci√≥n de Costos - AWS FREE TIER OPTIMIZADO

**Lambda Free Tier (permanente, no solo 12 meses):**
- ‚úÖ **1 mill√≥n de solicitudes gratuitas por mes**
- ‚úÖ **400,000 GB-segundos de tiempo de c√≥mputo por mes**
- ‚úÖ Suficiente para **cientos de ejecuciones diarias**

**Configuraci√≥n optimizada para este laboratorio:**
- Memoria: 512 MB (balance entre costo y rendimiento)
- Timeout: 5 minutos (m√°ximo para orquestaci√≥n)
- Ejecuciones estimadas: 50-100 por curso
- **Costo total: $0.00 USD** (100% dentro de Free Tier)

**Otros servicios relacionados:**
- ‚úÖ S3 Event Notifications: Gratis
- ‚úÖ CloudWatch Logs: 5 GB/mes gratis
- ‚úÖ EventBridge: Gratis para reglas b√°sicas
- ‚ö†Ô∏è Glue Jobs disparados: ~$0.10 por ejecuci√≥n

**Total estimado secci√≥n 2: $0-2 USD** (solo por Glue jobs)

---

## ‚öôÔ∏è PREREQUISITOS Y CONFIGURACI√ìN

### üìù Requisitos Previos

- ‚úÖ Completar Secci√≥n 1 (AWS Glue configurado)
- ‚úÖ Bucket S3 con estructura creada
- ‚úÖ AWS CLI configurado
- ‚úÖ Python 3.8+ con boto3
- ‚úÖ Familiaridad con eventos as√≠ncronos

---

## üöÄ PARTE 1: CONFIGURACI√ìN DE LAMBDA

### Paso 1.1: Crear Rol IAM para Lambda

**Usando la Consola AWS:**

1. Ve a **IAM** ‚Üí **Roles** ‚Üí **Create role**
2. **Trusted entity type**: AWS service
3. **Use case**: Lambda
4. Click **Next**
5. Busca y selecciona estas pol√≠ticas:
   - ‚úÖ `AWSLambdaBasicExecutionRole` (logs de CloudWatch)
   - ‚úÖ `AmazonS3FullAccess` (acceso a S3)
   - ‚úÖ `AWSGlueConsoleFullAccess` (iniciar Glue jobs)
6. Click **Next**
7. **Role name**: `LambdaDataPipelineRole`
8. Click **Create role**

**Usando AWS CLI (m√°s r√°pido):**

Crea el archivo `lambda-trust-policy.json`:

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": {
        "Service": "lambda.amazonaws.com"
      },
      "Action": "sts:AssumeRole"
    }
  ]
}
```

Ejecuta estos comandos:

```bash
# Crear el rol
aws iam create-role \
    --role-name LambdaDataPipelineRole \
    --assume-role-policy-document file://lambda-trust-policy.json \
    --profile glue-lab

# Adjuntar pol√≠ticas
aws iam attach-role-policy \
    --role-name LambdaDataPipelineRole \
    --policy-arn arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole \
    --profile glue-lab

aws iam attach-role-policy \
    --role-name LambdaDataPipelineRole \
    --policy-arn arn:aws:iam::aws:policy/AmazonS3FullAccess \
    --profile glue-lab

aws iam attach-role-policy \
    --role-name LambdaDataPipelineRole \
    --policy-arn arn:aws:iam::aws:policy/AWSGlueConsoleFullAccess \
    --profile glue-lab

# Crear pol√≠tica custom para permisos granulares
cat > lambda-glue-policy.json << 'EOF'
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "glue:StartJobRun",
        "glue:GetJobRun",
        "glue:GetJobRuns",
        "glue:BatchStopJobRun"
      ],
      "Resource": "*"
    }
  ]
}
EOF

aws iam put-role-policy \
    --role-name LambdaDataPipelineRole \
    --policy-name GlueJobExecutionPolicy \
    --policy-document file://lambda-glue-policy.json \
    --profile glue-lab

# Verificar
aws iam get-role --role-name LambdaDataPipelineRole --profile glue-lab
```

---

## üí° CONCEPTOS FUNDAMENTALES

### ¬øQu√© es AWS Lambda?

Lambda es un servicio de c√≥mputo **serverless** que:
- Ejecuta c√≥digo en respuesta a eventos
- Escala autom√°ticamente
- Cobra solo por tiempo de ejecuci√≥n
- No requiere gesti√≥n de servidores

### Patrones de Uso en Pipelines de Datos

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    PATRONES DE LAMBDA                       ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                             ‚îÇ
‚îÇ  1. EVENT-DRIVEN (Trigger por Evento)                      ‚îÇ
‚îÇ     S3 Upload ‚Üí Lambda ‚Üí Start Glue Job                    ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ  2. SCHEDULED (Ejecuci√≥n Programada)                       ‚îÇ
‚îÇ     EventBridge (cron) ‚Üí Lambda ‚Üí Orchestration            ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ  3. ORCHESTRATION (Orquestaci√≥n Compleja)                  ‚îÇ
‚îÇ     Lambda ‚Üí M√∫ltiples Jobs en Paralelo/Secuencial         ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ  4. DATA VALIDATION (Validaci√≥n Pre-ETL)                   ‚îÇ
‚îÇ     S3 Upload ‚Üí Lambda ‚Üí Validate ‚Üí Start ETL              ‚îÇ
‚îÇ                                                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üîß EJEMPLO 1: Lambda Trigger por S3 (Event-Driven)

### Caso de Uso Real: Auto-procesamiento de Archivos CSV

**Escenario**: Cada vez que se sube un archivo CSV de ventas a S3, autom√°ticamente:
1. Lambda valida el formato del archivo
2. Si es v√°lido, inicia un Glue Job para procesarlo
3. Env√≠a notificaci√≥n del resultado

### Paso 1.1: Crear la Funci√≥n Lambda

**C√≥digo: `s3_trigger_glue_job.py`**

```python
import json
import boto3
import os
from datetime import datetime
from urllib.parse import unquote_plus

# Clientes AWS
s3_client = boto3.client('s3')
glue_client = boto3.client('glue')

# Configuraci√≥n desde variables de entorno
GLUE_JOB_NAME = os.environ.get('GLUE_JOB_NAME', 'sales-etl-basic')
PROCESSED_PREFIX = os.environ.get('PROCESSED_PREFIX', 'processed/')

def lambda_handler(event, context):
    """
    Handler principal de Lambda que se activa cuando un archivo
    se sube a S3.
    
    Args:
        event: Evento S3 con informaci√≥n del archivo subido
        context: Contexto de Lambda con metadata de ejecuci√≥n
    
    Returns:
        dict: Respuesta con estado de la operaci√≥n
    """
    
    print(f"üöÄ Lambda iniciado: {context.function_name}")
    print(f"‚è∞ Timestamp: {datetime.now().isoformat()}")
    print(f"üì¶ Event: {json.dumps(event, indent=2)}")
    
    try:
        # Extraer informaci√≥n del evento S3
        record = event['Records'][0]
        bucket_name = record['s3']['bucket']['name']
        object_key = unquote_plus(record['s3']['object']['key'])
        file_size = record['s3']['object']['size']
        
        print(f"üìÅ Archivo detectado:")
        print(f"   Bucket: {bucket_name}")
        print(f"   Key: {object_key}")
        print(f"   Size: {file_size} bytes")
        
        # Validar que sea un archivo CSV en la carpeta correcta
        if not object_key.startswith('raw/sales/'):
            print(f"‚ö†Ô∏è  Archivo no est√° en raw/sales/, ignorando...")
            return {
                'statusCode': 200,
                'body': json.dumps('File not in target folder, skipping')
            }
        
        if not object_key.endswith('.csv'):
            print(f"‚ö†Ô∏è  Archivo no es CSV, ignorando...")
            return {
                'statusCode': 200,
                'body': json.dumps('Not a CSV file, skipping')
            }
        
        # Validaci√≥n b√°sica del archivo
        validation_result = validate_csv_file(bucket_name, object_key)
        
        if not validation_result['is_valid']:
            print(f"‚ùå Validaci√≥n fallida: {validation_result['error']}")
            
            # Mover archivo a carpeta de errores
            move_to_error_folder(bucket_name, object_key, validation_result['error'])
            
            return {
                'statusCode': 400,
                'body': json.dumps(f"Validation failed: {validation_result['error']}")
            }
        
        print(f"‚úÖ Archivo v√°lido, iniciando Glue Job...")
        
        # Iniciar Glue Job
        job_run_response = glue_client.start_job_run(
            JobName=GLUE_JOB_NAME,
            Arguments={
                '--INPUT_FILE': f's3://{bucket_name}/{object_key}',
                '--TIMESTAMP': datetime.now().isoformat(),
                '--enable-metrics': 'true',
                '--enable-continuous-cloudwatch-log': 'true'
            }
        )
        
        job_run_id = job_run_response['JobRunId']
        print(f"üéØ Glue Job iniciado: {job_run_id}")
        
        # Marcar archivo como procesado (agregar metadata)
        add_processing_metadata(bucket_name, object_key, job_run_id)
        
        return {
            'statusCode': 200,
            'body': json.dumps({
                'message': 'Glue job started successfully',
                'job_run_id': job_run_id,
                'file': object_key
            })
        }
        
    except Exception as e:
        print(f"‚ùå Error: {str(e)}")
        import traceback
        traceback.print_exc()
        
        return {
            'statusCode': 500,
            'body': json.dumps(f'Error: {str(e)}')
        }


def validate_csv_file(bucket_name, object_key):
    """
    Valida que el archivo CSV tenga el formato correcto.
    
    Args:
        bucket_name: Nombre del bucket S3
        object_key: Key del objeto en S3
    
    Returns:
        dict: Resultado de validaci√≥n con is_valid y error
    """
    try:
        # Descargar las primeras l√≠neas del archivo (optimizaci√≥n)
        response = s3_client.get_object(
            Bucket=bucket_name,
            Key=object_key,
            Range='bytes=0-1024'  # Solo primeros 1KB
        )
        
        content = response['Body'].read().decode('utf-8')
        lines = content.split('\n')
        
        if len(lines) < 2:
            return {
                'is_valid': False,
                'error': 'File has less than 2 lines (no data)'
            }
        
        # Validar headers esperados
        header = lines[0].strip()
        expected_columns = [
            'transaction_id', 'customer_id', 'product_id',
            'quantity', 'price', 'transaction_date'
        ]
        
        actual_columns = [col.strip() for col in header.split(',')]
        
        if actual_columns != expected_columns:
            return {
                'is_valid': False,
                'error': f'Invalid columns. Expected: {expected_columns}, Got: {actual_columns}'
            }
        
        # Validar que haya al menos una l√≠nea de datos
        if len(lines) < 2 or not lines[1].strip():
            return {
                'is_valid': False,
                'error': 'File has no data rows'
            }
        
        print(f"‚úÖ Validaci√≥n exitosa: {len(lines)-1} l√≠neas de datos")
        
        return {
            'is_valid': True,
            'error': None
        }
        
    except Exception as e:
        return {
            'is_valid': False,
            'error': f'Validation error: {str(e)}'
        }


def move_to_error_folder(bucket_name, object_key, error_message):
    """
    Mueve archivo inv√°lido a carpeta de errores.
    """
    try:
        # Crear nuevo key en carpeta de errores
        file_name = object_key.split('/')[-1]
        error_key = f'errors/{datetime.now().strftime("%Y%m%d")}/{file_name}'
        
        # Copiar archivo
        s3_client.copy_object(
            Bucket=bucket_name,
            CopySource={'Bucket': bucket_name, 'Key': object_key},
            Key=error_key,
            Metadata={
                'error': error_message,
                'original_key': object_key,
                'timestamp': datetime.now().isoformat()
            },
            MetadataDirective='REPLACE'
        )
        
        # Eliminar original
        s3_client.delete_object(Bucket=bucket_name, Key=object_key)
        
        print(f"üì¶ Archivo movido a: {error_key}")
        
    except Exception as e:
        print(f"‚ö†Ô∏è  Error moviendo archivo: {str(e)}")


def add_processing_metadata(bucket_name, object_key, job_run_id):
    """
    Agrega metadata de procesamiento al archivo S3.
    """
    try:
        s3_client.copy_object(
            Bucket=bucket_name,
            CopySource={'Bucket': bucket_name, 'Key': object_key},
            Key=object_key,
            Metadata={
                'processing_status': 'started',
                'glue_job_run_id': job_run_id,
                'processed_timestamp': datetime.now().isoformat()
            },
            MetadataDirective='REPLACE'
        )
        
        print(f"‚úÖ Metadata agregada al archivo")
        
    except Exception as e:
        print(f"‚ö†Ô∏è  Error agregando metadata: {str(e)}")
```

### Paso 1.2: Crear la Funci√≥n en AWS

**Opci√≥n A: Usando la Consola**

1. Ve a **Lambda** ‚Üí **Create function**
2. Selecciona **Author from scratch**
3. **Function name**: `S3-Glue-Trigger`
4. **Runtime**: Python 3.12
5. **Architecture**: x86_64
6. **Permissions**: Use existing role ‚Üí `LambdaDataPipelineRole`
7. Click **Create function**

8. En el editor de c√≥digo:
   - Borra el c√≥digo default
   - Pega el c√≥digo de `s3_trigger_glue_job.py`
   - Click **Deploy**

9. **Configuration** ‚Üí **General configuration** ‚Üí **Edit**:
   - **Memory**: 512 MB
   - **Timeout**: 5 minutes
   - Click **Save**

10. **Configuration** ‚Üí **Environment variables** ‚Üí **Edit**:
    - Key: `GLUE_JOB_NAME`, Value: `sales-etl-basic`
    - Key: `PROCESSED_PREFIX`, Value: `processed/`
    - Click **Save**

**Opci√≥n B: Usando AWS CLI (automatizado)**

Primero, empaqueta el c√≥digo:

```bash
# Crear directorio de deployment
mkdir lambda_deployment
cd lambda_deployment

# Copiar c√≥digo
cat > lambda_function.py << 'EOF'
# (Pegar aqu√≠ el c√≥digo completo de s3_trigger_glue_job.py)
EOF

# Crear ZIP
zip -r function.zip lambda_function.py

# Obtener ARN del rol
ROLE_ARN=$(aws iam get-role --role-name LambdaDataPipelineRole --profile glue-lab --query 'Role.Arn' --output text)

# Crear funci√≥n Lambda
aws lambda create-function \
    --function-name S3-Glue-Trigger \
    --runtime python3.12 \
    --role $ROLE_ARN \
    --handler lambda_function.lambda_handler \
    --zip-file fileb://function.zip \
    --timeout 300 \
    --memory-size 512 \
    --environment Variables="{GLUE_JOB_NAME=sales-etl-basic,PROCESSED_PREFIX=processed/}" \
    --profile glue-lab

echo "‚úÖ Lambda function created!"
```

### Paso 1.3: Configurar S3 Trigger

**Usando la Consola:**

1. En la funci√≥n Lambda, click en **Add trigger**
2. **Select a trigger**: S3
3. **Bucket**: Selecciona tu bucket (ej: `glue-lab-juan-12345`)
4. **Event type**: All object create events
5. **Prefix**: `raw/sales/`
6. **Suffix**: `.csv`
7. ‚úÖ Marcar: **I acknowledge that using the same S3 bucket...**
8. Click **Add**

**Usando AWS CLI:**

```bash
# Configurar variables
BUCKET_NAME="tu-bucket-aqui"
LAMBDA_ARN=$(aws lambda get-function --function-name S3-Glue-Trigger --profile glue-lab --query 'Configuration.FunctionArn' --output text)

# Dar permiso a S3 para invocar Lambda
aws lambda add-permission \
    --function-name S3-Glue-Trigger \
    --statement-id s3-trigger-permission \
    --action lambda:InvokeFunction \
    --principal s3.amazonaws.com \
    --source-arn arn:aws:s3:::$BUCKET_NAME \
    --profile glue-lab

# Crear configuraci√≥n de notificaci√≥n
cat > s3-notification.json << EOF
{
  "LambdaFunctionConfigurations": [
    {
      "LambdaFunctionArn": "$LAMBDA_ARN",
      "Events": ["s3:ObjectCreated:*"],
      "Filter": {
        "Key": {
          "FilterRules": [
            {"Name": "prefix", "Value": "raw/sales/"},
            {"Name": "suffix", "Value": ".csv"}
          ]
        }
      }
    }
  ]
}
EOF

# Aplicar configuraci√≥n
aws s3api put-bucket-notification-configuration \
    --bucket $BUCKET_NAME \
    --notification-configuration file://s3-notification.json \
    --profile glue-lab

echo "‚úÖ S3 trigger configured!"
```

### Paso 1.4: Probar la Integraci√≥n

```bash
# Subir archivo de prueba
echo "transaction_id,customer_id,product_id,quantity,price,transaction_date
TEST001,CUST999,PROD999,1,99.99,2024-12-15" > test_sales.csv

aws s3 cp test_sales.csv s3://$BUCKET_NAME/raw/sales/ --profile glue-lab

# Esperar 10 segundos y verificar logs
sleep 10

# Ver logs de Lambda
aws logs tail /aws/lambda/S3-Glue-Trigger --follow --profile glue-lab
```

---

## üé® EJEMPLO 2: Orquestaci√≥n Compleja con Step Functions

### Caso de Uso: Pipeline Multi-Etapa

**Flujo**:
1. Validar datos
2. Ejecutar ETL de ventas
3. Ejecutar ETL de clientes (en paralelo)
4. Combinar resultados
5. Generar reporte

### Lambda para Orquestaci√≥n: `orchestrator_lambda.py`

```python
import json
import boto3
import time
from datetime import datetime

glue_client = boto3.client('glue')
s3_client = boto3.client('s3')

def lambda_handler(event, context):
    """
    Orquesta la ejecuci√≥n de m√∫ltiples Glue Jobs en secuencia.
    """
    
    pipeline_id = event.get('pipeline_id', f"pipeline-{int(time.time())}")
    bucket_name = event.get('bucket_name')
    
    print(f"üé≠ Iniciando orquestaci√≥n: {pipeline_id}")
    
    results = {
        'pipeline_id': pipeline_id,
        'started_at': datetime.now().isoformat(),
        'jobs': []
    }
    
    try:
        # Etapa 1: Validar datos crudos
        print("üìä Etapa 1: Validando datos...")
        validation_result = validate_raw_data(bucket_name)
        
        if not validation_result['valid']:
            raise Exception(f"Validation failed: {validation_result['errors']}")
        
        results['validation'] = validation_result
        
        # Etapa 2: Ejecutar Jobs de Glue en paralelo
        print("üöÄ Etapa 2: Iniciando Glue Jobs...")
        
        jobs_to_run = [
            {
                'name': 'sales-etl-basic',
                'description': 'Procesar ventas'
            },
            {
                'name': 'customer-etl',  # Asumiendo que existe
                'description': 'Procesar clientes'
            }
        ]
        
        job_run_ids = []
        
        for job_config in jobs_to_run:
            try:
                response = glue_client.start_job_run(
                    JobName=job_config['name'],
                    Arguments={
                        '--pipeline_id': pipeline_id,
                        '--enable-metrics': 'true'
                    }
                )
                
                job_run_id = response['JobRunId']
                job_run_ids.append({
                    'job_name': job_config['name'],
                    'job_run_id': job_run_id,
                    'description': job_config['description']
                })
                
                print(f"  ‚úÖ {job_config['name']}: {job_run_id}")
                
            except Exception as e:
                print(f"  ‚ùå Error en {job_config['name']}: {str(e)}")
                job_run_ids.append({
                    'job_name': job_config['name'],
                    'error': str(e)
                })
        
        results['jobs'] = job_run_ids
        
        # Etapa 3: Monitorear progreso (non-blocking)
        print("‚è≥ Etapa 3: Jobs iniciados, monitoreo as√≠ncrono...")
        
        results['status'] = 'RUNNING'
        results['message'] = f"Pipeline {pipeline_id} iniciado con {len(job_run_ids)} jobs"
        
        # Guardar metadata del pipeline
        save_pipeline_metadata(bucket_name, pipeline_id, results)
        
        return {
            'statusCode': 200,
            'body': json.dumps(results, default=str)
        }
        
    except Exception as e:
        print(f"‚ùå Error en orquestaci√≥n: {str(e)}")
        
        results['status'] = 'FAILED'
        results['error'] = str(e)
        
        return {
            'statusCode': 500,
            'body': json.dumps(results, default=str)
        }


def validate_raw_data(bucket_name):
    """
    Valida que todos los archivos requeridos existan.
    """
    required_files = [
        'raw/sales/sales_data.csv',
        'raw/customers/customers_data.csv'
    ]
    
    validation_result = {
        'valid': True,
        'errors': [],
        'files_checked': []
    }
    
    for file_key in required_files:
        try:
            s3_client.head_object(Bucket=bucket_name, Key=file_key)
            validation_result['files_checked'].append({
                'file': file_key,
                'status': 'EXISTS'
            })
        except:
            validation_result['valid'] = False
            validation_result['errors'].append(f"Missing file: {file_key}")
            validation_result['files_checked'].append({
                'file': file_key,
                'status': 'MISSING'
            })
    
    return validation_result


def save_pipeline_metadata(bucket_name, pipeline_id, metadata):
    """
    Guarda metadata del pipeline en S3.
    """
    try:
        key = f"pipelines/{pipeline_id}/metadata.json"
        
        s3_client.put_object(
            Bucket=bucket_name,
            Key=key,
            Body=json.dumps(metadata, indent=2, default=str),
            ContentType='application/json'
        )
        
        print(f"üíæ Metadata guardada en: s3://{bucket_name}/{key}")
        
    except Exception as e:
        print(f"‚ö†Ô∏è  Error guardando metadata: {str(e)}")
```

---

## üîî EJEMPLO 3: Scheduled Lambda (Ejecuci√≥n Programada)

### Caso de Uso: Reporte Diario Autom√°tico

**Lambda para reportes programados: `scheduled_report.py`**

```python
import json
import boto3
from datetime import datetime, timedelta

s3_client = boto3.client('s3')
glue_client = boto3.client('glue')

def lambda_handler(event, context):
    """
    Se ejecuta diariamente a las 2 AM para generar reporte del d√≠a anterior.
    """
    
    print(f"üìÖ Generando reporte diario: {datetime.now().isoformat()}")
    
    # Calcular fecha del reporte (ayer)
    report_date = datetime.now() - timedelta(days=1)
    date_str = report_date.strftime('%Y-%m-%d')
    
    print(f"üìä Fecha del reporte: {date_str}")
    
    try:
        # Iniciar job de agregaci√≥n diaria
        response = glue_client.start_job_run(
            JobName='daily-aggregation-job',
            Arguments={
                '--report_date': date_str,
                '--enable-metrics': 'true'
            }
        )
        
        job_run_id = response['JobRunId']
        print(f"‚úÖ Job de agregaci√≥n iniciado: {job_run_id}")
        
        return {
            'statusCode': 200,
            'body': json.dumps({
                'message': 'Daily report job started',
                'date': date_str,
                'job_run_id': job_run_id
            })
        }
        
    except Exception as e:
        print(f"‚ùå Error: {str(e)}")
        return {
            'statusCode': 500,
            'body': json.dumps({'error': str(e)})
        }
```

### Configurar EventBridge Rule (Cron)

```bash
# Crear regla de EventBridge para ejecuci√≥n diaria a las 2 AM
aws events put-rule \
    --name DailyReportTrigger \
    --schedule-expression "cron(0 2 * * ? *)" \
    --state ENABLED \
    --description "Trigger daily report generation at 2 AM" \
    --profile glue-lab

# Obtener ARN de Lambda
LAMBDA_ARN=$(aws lambda get-function --function-name scheduled-report --profile glue-lab --query 'Configuration.FunctionArn' --output text)

# Dar permiso a EventBridge
aws lambda add-permission \
    --function-name scheduled-report \
    --statement-id eventbridge-daily-trigger \
    --action lambda:InvokeFunction \
    --principal events.amazonaws.com \
    --source-arn arn:aws:events:us-east-1:$(aws sts get-caller-identity --query Account --output text):rule/DailyReportTrigger \
    --profile glue-lab

# Agregar Lambda como target
aws events put-targets \
    --rule DailyReportTrigger \
    --targets "Id"="1","Arn"="$LAMBDA_ARN" \
    --profile glue-lab

echo "‚úÖ Scheduled trigger configured!"
```

---

## üìù EJERCICIO PR√ÅCTICO 2: Sistema de Monitoreo y Alertas

### Objetivo

Crear un sistema que:
1. Monitorea el estado de Glue Jobs
2. Detecta fallos y reintentos autom√°ticamente
3. Env√≠a notificaciones por SNS (opcional)

### Requisitos

**Lambda: `job_monitor.py`**

```python
import json
import boto3
import time
from datetime import datetime

glue_client = boto3.client('glue')
# sns_client = boto3.client('sns')  # Descomentar si usas SNS

def lambda_handler(event, context):
    """
    Monitorea el estado de un Glue Job y reintenta si falla.
    
    Event debe contener:
    {
        "job_name": "sales-etl-basic",
        "job_run_id": "jr_xxx",
        "retry_count": 0
    }
    """
    
    job_name = event['job_name']
    job_run_id = event['job_run_id']
    retry_count = event.get('retry_count', 0)
    max_retries = 3
    
    print(f"üîç Monitoreando Job: {job_name}")
    print(f"   Run ID: {job_run_id}")
    print(f"   Retry: {retry_count}/{max_retries}")
    
    try:
        # Obtener estado del job
        response = glue_client.get_job_run(
            JobName=job_name,
            RunId=job_run_id
        )
        
        job_run = response['JobRun']
        status = job_run['JobRunState']
        
        print(f"üìä Estado actual: {status}")
        
        # Estados posibles: STARTING, RUNNING, STOPPING, STOPPED, SUCCEEDED, FAILED, TIMEOUT
        
        if status == 'SUCCEEDED':
            print("‚úÖ Job completado exitosamente!")
            
            return {
                'statusCode': 200,
                'status': 'SUCCEEDED',
                'job_name': job_name,
                'job_run_id': job_run_id,
                'message': 'Job completed successfully'
            }
        
        elif status in ['FAILED', 'TIMEOUT', 'STOPPED']:
            error_message = job_run.get('ErrorMessage', 'Unknown error')
            print(f"‚ùå Job fall√≥: {error_message}")
            
            # Reintentar si no se alcanz√≥ el m√°ximo
            if retry_count < max_retries:
                print(f"üîÑ Reintentando... ({retry_count + 1}/{max_retries})")
                
                # Iniciar nuevo job run
                retry_response = glue_client.start_job_run(
                    JobName=job_name,
                    Arguments={
                        '--retry_attempt': str(retry_count + 1),
                        '--original_run_id': job_run_id,
                        '--enable-metrics': 'true'
                    }
                )
                
                new_job_run_id = retry_response['JobRunId']
                print(f"üÜï Nuevo job iniciado: {new_job_run_id}")
                
                # Enviar notificaci√≥n de reintento (descomentar si usas SNS)
                # send_notification(
                #     f"Job {job_name} fall√≥, reintentando ({retry_count + 1}/{max_retries})",
                #     error_message
                # )
                
                return {
                    'statusCode': 202,
                    'status': 'RETRYING',
                    'job_name': job_name,
                    'original_run_id': job_run_id,
                    'new_run_id': new_job_run_id,
                    'retry_count': retry_count + 1,
                    'message': f'Retrying job (attempt {retry_count + 1})'
                }
            else:
                print(f"üö´ M√°ximo de reintentos alcanzado")
                
                # Enviar alerta cr√≠tica
                # send_notification(
                #     f"CRITICAL: Job {job_name} fall√≥ despu√©s de {max_retries} reintentos",
                #     error_message
                # )
                
                return {
                    'statusCode': 500,
                    'status': 'FAILED',
                    'job_name': job_name,
                    'job_run_id': job_run_id,
                    'retry_count': retry_count,
                    'error': error_message,
                    'message': 'Job failed after max retries'
                }
        
        else:  # STARTING, RUNNING, STOPPING
            print(f"‚è≥ Job a√∫n en progreso...")
            
            return {
                'statusCode': 202,
                'status': status,
                'job_name': job_name,
                'job_run_id': job_run_id,
                'message': 'Job still in progress'
            }
    
    except Exception as e:
        print(f"‚ùå Error monitoreando job: {str(e)}")
        
        return {
            'statusCode': 500,
            'error': str(e),
            'message': 'Error monitoring job'
        }


def send_notification(subject, message):
    """
    Env√≠a notificaci√≥n por SNS (opcional).
    """
    try:
        sns_topic_arn = 'arn:aws:sns:us-east-1:ACCOUNT_ID:data-pipeline-alerts'
        
        # sns_client.publish(
        #     TopicArn=sns_topic_arn,
        #     Subject=subject,
        #     Message=message
        # )
        
        print(f"üìß Notificaci√≥n enviada: {subject}")
        
    except Exception as e:
        print(f"‚ö†Ô∏è  Error enviando notificaci√≥n: {str(e)}")
```

### Plantilla de Soluci√≥n

**Tu tarea**: Implementar la funci√≥n completa con:

1. ‚úÖ Monitoreo de estado de job
2. ‚úÖ L√≥gica de reintentos (m√°ximo 3)
3. ‚úÖ Logging detallado
4. ‚úÖ Manejo de errores robusto
5. ‚≠ê BONUS: Integraci√≥n con SNS para alertas

### Criterios de Evaluaci√≥n

- ‚úÖ Manejo correcto de todos los estados de Glue
- ‚úÖ Implementaci√≥n de reintentos con backoff
- ‚úÖ Logging claro y estructurado
- ‚úÖ Manejo de errores y casos edge
- ‚úÖ Optimizaci√≥n para Free Tier

---

## üéì MEJORES PR√ÅCTICAS

### 1. Manejo de Errores y Reintentos

```python
import time
from functools import wraps

def retry_with_backoff(max_retries=3, backoff_factor=2):
    """
    Decorator para reintentar operaciones con exponential backoff.
    """
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            for attempt in range(max_retries):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    if attempt == max_retries - 1:
                        raise
                    
                    wait_time = backoff_factor ** attempt
                    print(f"‚ö†Ô∏è  Intento {attempt + 1} fall√≥, esperando {wait_time}s...")
                    time.sleep(wait_time)
            
        return wrapper
    return decorator

# Uso
@retry_with_backoff(max_retries=3, backoff_factor=2)
def start_glue_job(job_name):
    return glue_client.start_job_run(JobName=job_name)
```

### 2. Logging Estructurado

```python
import json
import logging

# Configurar logger
logger = logging.getLogger()
logger.setLevel(logging.INFO)

def log_event(event_type, details):
    """
    Logging estructurado para mejor an√°lisis en CloudWatch.
    """
    log_entry = {
        'timestamp': datetime.now().isoformat(),
        'event_type': event_type,
        'details': details
    }
    
    logger.info(json.dumps(log_entry))

# Uso
log_event('JOB_STARTED', {
    'job_name': 'sales-etl-basic',
    'trigger': 's3_upload'
})
```

### 3. Optimizaci√≥n de Memoria

```python
def lambda_handler(event, context):
    """
    Monitorea uso de memoria para optimizar costos.
    """
    import sys
    
    # Obtener memoria disponible
    memory_limit = int(context.memory_limit_in_mb)
    memory_used = sys.getsizeof(event) / (1024 * 1024)
    
    print(f"üíæ Memoria - L√≠mite: {memory_limit}MB, Usado: {memory_used:.2f}MB")
    
    # Tu c√≥digo aqu√≠...
    
    # Si consistentemente usas < 256MB, reduce la configuraci√≥n
    if memory_used < memory_limit * 0.3:
        print(f"üí° Considera reducir memoria a {int(memory_limit/2)}MB")
```

### 4. Idempotencia

```python
def lambda_handler(event, context):
    """
    Lambda idempotente - puede ejecutarse m√∫ltiples veces sin efectos adversos.
    """
    
    # Generar request_id √∫nico basado en el evento
    import hashlib
    event_hash = hashlib.md5(
        json.dumps(event, sort_keys=True).encode()
    ).hexdigest()
    
    request_id = f"req-{event_hash[:8]}"
    
    # Verificar si ya se proces√≥
    try:
        s3_client.head_object(
            Bucket=bucket_name,
            Key=f'processed/{request_id}.json'
        )
        
        print(f"‚ö†Ô∏è  Request {request_id} ya procesado, omitiendo...")
        return {
            'statusCode': 200,
            'body': json.dumps({'message': 'Already processed'})
        }
        
    except s3_client.exceptions.NoSuchKey:
        # Continuar con procesamiento...
        pass
    
    # Marcar como procesado al finalizar
    s3_client.put_object(
        Bucket=bucket_name,
        Key=f'processed/{request_id}.json',
        Body=json.dumps({'processed_at': datetime.now().isoformat()})
    )
```

### 5. Variables de Entorno Seguras

```python
import os

# ‚úÖ BUENAS PR√ÅCTICAS
BUCKET_NAME = os.environ.get('BUCKET_NAME')
GLUE_JOB_NAME = os.environ.get('GLUE_JOB_NAME')

if not BUCKET_NAME or not GLUE_JOB_NAME:
    raise ValueError("Required environment variables not set")

# ‚ùå EVITAR
BUCKET_NAME = "hardcoded-bucket-name"  # No hacer esto!
```

---

## üìä MONITOREO Y M√âTRICAS

### Dashboard de CloudWatch

Crea un dashboard para monitorear tus Lambdas:

```bash
# Crear dashboard de CloudWatch
cat > dashboard.json << 'EOF'
{
  "widgets": [
    {
      "type": "metric",
      "properties": {
        "metrics": [
          ["AWS/Lambda", "Invocations", {"stat": "Sum"}],
          [".", "Errors", {"stat": "Sum"}],
          [".", "Duration", {"stat": "Average"}]
        ],
        "period": 300,
        "stat": "Average",
        "region": "us-east-1",
        "title": "Lambda Metrics"
      }
    }
  ]
}
EOF

aws cloudwatch put-dashboard \
    --dashboard-name DataPipelineDashboard \
    --dashboard-body file://dashboard.json \
    --profile glue-lab
```

### Consultas de CloudWatch Insights

```bash
# Ver errores recientes
aws logs tail /aws/lambda/S3-Glue-Trigger \
    --filter-pattern "ERROR" \
    --since 1h \
    --profile glue-lab

# An√°lisis de performance
aws logs insights query \
    --log-group-name /aws/lambda/S3-Glue-Trigger \
    --start-time $(date -d '1 hour ago' +%s) \
    --end-time $(date +%s) \
    --query-string 'fields @timestamp, @message | filter @type = "REPORT" | stats avg(@duration), max(@duration), min(@duration)' \
    --profile glue-lab
```

---

## üß™ TESTING LOCAL

### Probar Lambda Localmente

Instala AWS SAM CLI:

```bash
# Instalar SAM CLI
pip install aws-sam-cli

# Crear evento de prueba
cat > test_event.json << 'EOF'
{
  "Records": [
    {
      "s3": {
        "bucket": {"name": "tu-bucket"},
        "object": {"key": "raw/sales/test.csv", "size": 1024}
      }
    }
  ]
}
EOF

# Invocar Lambda localmente
sam local invoke S3-Glue-Trigger --event test_event.json
```

### Unit Tests con pytest

```python
# test_lambda.py
import pytest
import json
from unittest.mock import Mock, patch
import lambda_function

def test_validate_csv_valid():
    """Test validaci√≥n de CSV v√°lido"""
    
    with patch('lambda_function.s3_client') as mock_s3:
        # Mock respuesta de S3
        mock_s3.get_object.return_value = {
            'Body': Mock(read=lambda: b'transaction_id,customer_id\nTXN001,CUST123')
        }
        
        result = lambda_function.validate_csv_file('bucket', 'key')
        
        assert result['is_valid'] == True
        assert result['error'] is None

def test_lambda_handler_invalid_file():
    """Test handler con archivo inv√°lido"""
    
    event = {
        'Records': [{
            's3': {
                'bucket': {'name': 'test-bucket'},
                'object': {'key': 'raw/sales/invalid.txt', 'size': 100}
            }
        }]
    }
    
    result = lambda_function.lambda_handler(event, None)
    
    assert result['statusCode'] == 200
    assert 'skipping' in result['body']

# Ejecutar tests
# pytest test_lambda.py -v
```

---

## üßπ LIMPIEZA DE RECURSOS

### Al Terminar Cada Sesi√≥n

```bash
# Script: cleanup_lambda.sh

#!/bin/bash
PROFILE="glue-lab"

echo "üßπ Limpiando recursos de Lambda..."

# 1. Eliminar triggers de EventBridge
aws events remove-targets \
    --rule DailyReportTrigger \
    --ids "1" \
    --profile $PROFILE 2>/dev/null

aws events delete-rule \
    --name DailyReportTrigger \
    --profile $PROFILE 2>/dev/null

# 2. Eliminar notificaciones de S3
BUCKET_NAME="tu-bucket-aqui"
aws s3api put-bucket-notification-configuration \
    --bucket $BUCKET_NAME \
    --notification-configuration '{}' \
    --profile $PROFILE 2>/dev/null

# 3. NO eliminar funciones Lambda (r√°pido recrearlas si necesitas)
# Solo si quieres limpieza completa:
# aws lambda delete-function --function-name S3-Glue-Trigger --profile $PROFILE

echo "‚úÖ Limpieza completada"
```

### Verificar Uso de Free Tier

```bash
# Ver invocaciones de Lambda del mes
aws cloudwatch get-metric-statistics \
    --namespace AWS/Lambda \
    --metric-name Invocations \
    --dimensions Name=FunctionName,Value=S3-Glue-Trigger \
    --start-time $(date -d '30 days ago' -u +%Y-%m-%dT%H:%M:%S) \
    --end-time $(date -u +%Y-%m-%dT%H:%M:%S) \
    --period 2592000 \
    --statistics Sum \
    --profile glue-lab

# Calcular GB-segundos usados
aws cloudwatch get-metric-statistics \
    --namespace AWS/Lambda \
    --metric-name Duration \
    --dimensions Name=FunctionName,Value=S3-Glue-Trigger \
    --start-time $(date -d '30 days ago' -u +%Y-%m-%dT%H:%M:%S) \
    --end-time $(date -u +%Y-%m-%dT%H:%M:%S) \
    --period 2592000 \
    --statistics Sum \
    --profile glue-lab
```

---

## üìö RECURSOS ADICIONALES

### Documentaci√≥n Oficial
- [AWS Lambda Developer Guide](https://docs.aws.amazon.com/lambda/)
- [Lambda Best Practices](https://docs.aws.amazon.com/lambda/latest/dg/best-practices.html)
- [EventBridge Scheduler](https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-create-rule-schedule.html)

### Herramientas √ötiles
- [AWS SAM CLI](https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/serverless-sam-cli-install.html)
- [Lambda Powertools Python](https://awslabs.github.io/aws-lambda-powertools-python/)
- [Serverless Framework](https://www.serverless.com/)

### Patrones Avanzados
- [Lambda Destinations](https://docs.aws.amazon.com/lambda/latest/dg/invocation-async.html#invocation-async-destinations)
- [Step Functions con Lambda](https://docs.aws.amazon.com/step-functions/latest/dg/concepts-standard-vs-express.html)
- [Lambda Layers](https://docs.aws.amazon.com/lambda/latest/dg/configuration-layers.html)

---

## üéØ RESUMEN DE LA SECCI√ìN

### ‚úÖ Has Aprendido:

1. **Event-Driven Architecture**
   - Triggers de S3
   - Validaci√≥n autom√°tica de datos
   - Integraci√≥n Lambda ‚Üí Glue

2. **Orquestaci√≥n**
   - Ejecuci√≥n de m√∫ltiples jobs
   - Manejo de dependencias
   - Metadata tracking

3. **Scheduled Jobs**
   - EventBridge rules
   - Cron expressions
   - Reportes autom√°ticos

4. **Monitoring & Alerting**
   - Estado de jobs
   - Reintentos autom√°ticos
   - CloudWatch metrics

5. **Best Practices**
   - Idempotencia
   - Error handling
   - Cost optimization
   - Testing

### üí∞ Costo Total Secci√≥n 2:
- Lambda invocations: **$0.00** (Free Tier)
- CloudWatch Logs: **$0.00** (Free Tier)
- Glue jobs disparados: **~$1-2 USD**
- **Total: $1-2 USD**

---

## üîÑ Transici√≥n a Secci√≥n 3

En la siguiente secci√≥n, aprenderemos:
- **Optimizaci√≥n de performance** de Glue jobs
- **Estrategias de particionamiento** avanzadas
- **Compresi√≥n y formatos** de datos
- **Monitoring y debugging** de pipelines
- **Cost optimization** a escala

**¬øListo para la Secci√≥n 3: Optimizaci√≥n de Pipelines?** üöÄ